{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu8Ves1vVTZnLF+hKWN0ZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ovocode05/NeuroDeff/blob/main/pybase_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN from Scratch (Single Layer Perceptron) OR AND Logic Gates"
      ],
      "metadata": {
        "id": "nbnvziDxI3Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "b_EjPyGWJ3MT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L0McgAaI0kP"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, input_size):\n",
        "    #xavier normalization for sigmoid function\n",
        "    self.weights = np.random.normal(0, np.sqrt(2/(input_size+1)), input_size)\n",
        "    self.bias = np.zeros(1)\n",
        "\n",
        "  #functions\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def tanh(self, x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # return self.sigmoid(np.dot(self.weights, x) + self.bias)\n",
        "    return self.tanh(np.dot(self.weights, x) + self.bias)\n",
        "\n",
        "  def MSE_derivative(self, y, y_pred):\n",
        "    # return y_pred*(1-y_pred)*(y-y_pred) #Mse + sigmoid\n",
        "    return (1 - y_pred**2) * (y - y_pred) #Mse + tanh\n",
        "\n",
        "  #training\n",
        "  def train(self, x, Y, lr=0.05, epochs=1000):\n",
        "    for _ in range(epochs):\n",
        "      total_loss= 0\n",
        "      for xi, yi in zip(x,Y):\n",
        "        y_pred = self.forward(xi) #range of y_pred [0,1]\n",
        "        error = self.MSE_derivative(yi, y_pred)\n",
        "        # error = yi-y_pred\n",
        "\n",
        "        self.weights += lr*error*xi\n",
        "        self.bias += lr*error\n",
        "\n",
        "        self.weights = np.clip(self.weights, -10, 10)\n",
        "        self.bias = np.clip(self.bias, -10, 10)\n",
        "    return self.weights, self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_or = np.array([[0,0], [0,1],[1,0],[1,1]])\n",
        "Y_or = np.array([-1,1,1,1])\n",
        "neuron = Neuron(input_size = 2)\n",
        "print(neuron.train(X_or,Y_or))\n",
        "print([1 if neuron.forward(x)> 0 else -1 for x in X_or])\n",
        "\n",
        "#and gate logic\n",
        "X_and = np.array([[0,0], [0,1],[1,0],[1,1]])\n",
        "Y_and = np.array([-1,-1,-1,1])\n",
        "neuron2 = Neuron(input_size = 2)\n",
        "print(neuron2.train(X_and,Y_and))\n",
        "print([1 if neuron2.forward(x)> 0 else -1 for x in X_and])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEiNeNgaN5Li",
        "outputId": "7c05413e-1664-4c63-9806-e800ae706456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([2.96029645, 2.96142974]), array([-1.35546999]))\n",
            "[-1, 1, 1, 1]\n",
            "(array([2.62007346, 2.61824336]), array([-3.97228672]))\n",
            "[-1, -1, -1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Layer Perceptron (XOR)"
      ],
      "metadata": {
        "id": "YlGh5hcjs6OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron_Network:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    # Xavier Normalization\n",
        "    self.weights1 = np.random.normal(0, np.sqrt(2/(input_size + hidden_size)), (input_size, hidden_size))\n",
        "    self.bias1 = np.zeros(hidden_size)\n",
        "    self.weights2 = np.random.normal(0, np.sqrt(2/(hidden_size + output_size)), (hidden_size,output_size))\n",
        "    self.bias2 = np.zeros(output_size)\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    return x*(1-x)\n",
        "\n",
        "  def Relu(self, x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def Relu_derivative(self, x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.h_input = np.dot(x, self.weights1) + self.bias1 #hidden layer input\n",
        "    self.h_output = self.Relu(self.h_input) #hidden layer output\n",
        "\n",
        "    self.y_input = np.dot(self.h_output ,self.weights2) + self.bias2 #output layer input\n",
        "    self.y_pred = self.sigmoid(self.y_input) #output layer output\n",
        "    return self.y_pred\n",
        "\n",
        "  def binary_CE(self,y, y_pred):\n",
        "    return -np.sum(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
        "\n",
        "  def train(self, x, Y, lr=0.01, epoch=10000):\n",
        "    for _ in range(epoch):\n",
        "      y_pred = self.forward(x)\n",
        "      error = y_pred - Y\n",
        "\n",
        "      #output layer gradient\n",
        "      d_output = error*self.sigmoid_derivative(y_pred)\n",
        "      grad_w2 = np.dot(self.h_output.T, d_output)\n",
        "      grad_b2 = np.sum(d_output, axis=0)\n",
        "\n",
        "      #hidden layer gradient\n",
        "      d_hidden = np.dot(d_output, self.weights2.T)*self.Relu_derivative(self.h_output)\n",
        "      grad_w1 = np.dot(x.T, d_hidden)\n",
        "      grad_b1 = np.sum(d_hidden, axis=0)\n",
        "\n",
        "      #update weights\n",
        "      self.weights1 -= lr*grad_w1\n",
        "      self.bias1 -= lr*grad_b1\n",
        "      self.weights2 -= lr*grad_w2\n",
        "      self.bias2 -= lr*grad_b2\n",
        "\n",
        "    return self.weights1, self.bias1, self.weights2, self.bias2\n",
        "    #relu shows faster convergence, derivatives either 0 or 1\n",
        "    #in case sigmoid weight updates are negligible during back propagation"
      ],
      "metadata": {
        "id": "sYR4g7qmtEcC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "neuron3 = Neuron_Network(2,2,1)\n",
        "neuron3.train(X,y)\n",
        "# Test predictions\n",
        "print(\"XOR Predictions:\")\n",
        "print([0 if neuron3.forward(x) < 0.5 else 1 for x in X])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXDjk4AT4plT",
        "outputId": "3df3c9d4-00d5-4580-f8fa-cac88723217b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Predictions:\n",
            "[0, 1, 1, 0]\n"
          ]
        }
      ]
    }
  ]
}